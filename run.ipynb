{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 17.18 GB\n",
      "Number of cores: 8\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "total_memory = psutil.virtual_memory().total / 1e9  # Convert bytes to GB\n",
    "print(f\"Total memory: {total_memory:.2f} GB\")\n",
    "\n",
    "import os\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Number of cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abinthomas/.pyenv/versions/3.12.3/lib/python3.12/contextlib.py:144: UserWarning: Creating scratch directories is taking a surprisingly long time. (1.58s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:54410' processes=4 threads=8, memory=16.00 GiB>\n",
      "http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4, threads_per_worker=2\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "print(client)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import logging\n",
    "\n",
    "# Enable debug-level logging for Dask\n",
    "# logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abinthomas/.pyenv/versions/3.12.3/envs/vgm/lib/python3.12/site-packages/dask/base.py:1103: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 10\t time lapse 69.92864s\t ll change 139955.00983\n",
      "  Iteration 20\t time lapse 30.66324s\t ll change 54669.25238\n",
      "  Iteration 30\t time lapse 32.52059s\t ll change 26047.94393\n",
      "  Iteration 40\t time lapse 31.52468s\t ll change 15615.08877\n",
      "  Iteration 50\t time lapse 32.84570s\t ll change 12189.85846\n",
      "  Iteration 60\t time lapse 33.04623s\t ll change 7656.67696\n",
      "  Iteration 70\t time lapse 35.66511s\t ll change 5621.18725\n",
      "  Iteration 80\t time lapse 35.65871s\t ll change 5081.47197\n",
      "  Iteration 90\t time lapse 37.03756s\t ll change 5847.71793\n",
      "  Iteration 100\t time lapse 35.66207s\t ll change 10763.49993\n",
      "Initialization did not converge. time lapse 374.55262s\t lower bound -42235135.65552.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abinthomas/Desktop/vgm/dbgm/dbgm.py:167: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 41s, sys: 17.1 s, total: 4min 58s\n",
      "Wall time: 6min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"Fit the BGM model on 10M rows of data\"\"\"\n",
    "\n",
    "from data_transformer import DistributedDataTransformer\n",
    "\n",
    "n_files = 200\n",
    "num_partitions = 20\n",
    "\n",
    "# Not worth doing this in dask\n",
    "pd.concat([pd.read_csv(\"assets/Credit.csv\")[[\"Amount\"]]]*(n_files//num_partitions), ignore_index=True).to_csv(\"assets/Credit_large.csv\", index=False)\n",
    "\n",
    "train_data = dd.read_csv([\"assets/Credit_large.csv\" for _ in range(num_partitions)])[[\"Amount\"]].repartition(npartitions=num_partitions)\n",
    "\n",
    "transformer = DistributedDataTransformer(train_data)\n",
    "transformer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# \"\"\"Transform and inverse transform 10M rows of data\"\"\"\n",
    "\n",
    "# transformed_train_data = transformer.transform(\n",
    "#     train_data.to_dask_array(lengths=True)\n",
    "# )\n",
    "# inverse_transformed_train_data = transformer.inverse_transform(\n",
    "#     transformed_train_data\n",
    "# )\n",
    "# regen_df = dd.from_dask_array(inverse_transformed_train_data, columns=[\"Amount\"])\n",
    "\n",
    "# print(dd.from_dask_array(da.abs(train_data.to_dask_array(lengths=True) - regen_df.to_dask_array(lengths=True)))[0].describe().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abinthomas/.pyenv/versions/3.12.3/envs/vgm/lib/python3.12/site-packages/distributed/client.py:3371: UserWarning: Sending large graph of size 11.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    9.968400e+08\n",
      "mean     3.389181e+00\n",
      "std      1.245775e+02\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.177351e+04\n",
      "Name: 0, dtype: float64\n",
      "CPU times: user 6min 15s, sys: 42.8 s, total: 6min 58s\n",
      "Wall time: 10min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"Transform and inverse transform 1B rows of data\"\"\"\n",
    "\n",
    "n_files = 20000\n",
    "num_partitions = 2000\n",
    "\n",
    "# Not worth doing this in dask\n",
    "pd.concat([pd.read_csv(\"assets/Credit.csv\")[[\"Amount\"]]]*(n_files//num_partitions), ignore_index=True).to_csv(\"assets/Credit_large.csv\", index=False)\n",
    "\n",
    "train_data = dd.read_csv([\"assets/Credit_large.csv\" for _ in range(num_partitions)])[[\"Amount\"]].repartition(npartitions=num_partitions)\n",
    "\n",
    "transformed_train_data = transformer.transform(\n",
    "    train_data.to_dask_array(lengths=True)\n",
    ")\n",
    "inverse_transformed_train_data = transformer.inverse_transform(\n",
    "    transformed_train_data\n",
    ")\n",
    "regen_df = dd.from_dask_array(inverse_transformed_train_data, columns=[\"Amount\"])\n",
    "\n",
    "print(dd.from_dask_array(da.abs(train_data.to_dask_array(lengths=True) - regen_df.to_dask_array(lengths=True)))[0].describe().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 05:04:12,248 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abinthomas/.pyenv/versions/3.12.3/envs/vgm/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abinthomas/.pyenv/versions/3.12.3/envs/vgm/lib/python3.12/site-packages/distributed/worker.py\", line 1269, in heartbeat\n",
      "    response = await retry_operation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abinthomas/.pyenv/versions/3.12.3/envs/vgm/lib/python3.12/site-packages/distributed/utils_comm.py\", line 441, in retry_operation\n",
      "    return await retry(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/abinthomas/.pyenv/versions/3.12.3/envs/vgm/lib/python3.12/site-packages/distributed/utils_comm.py\", line 420, in retry\n",
      "    return await coro()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/abinthomas/.pyenv/versions/3.12.3/envs/vgm/lib/python3.12/site-packages/distributed/core.py\", line 1259, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abinthomas/.pyenv/versions/3.12.3/envs/vgm/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abinthomas/.pyenv/versions/3.12.3/envs/vgm/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Users/abinthomas/.pyenv/versions/3.12.3/envs/vgm/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:54438 remote=tcp://127.0.0.1:54410>: Stream is closed\n"
     ]
    }
   ],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vgm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
